\setcounter{section}{-1}
\section{Comprehension questions}
Procedures for minimizing functions of the form $f : \mathbb{R}^N \rightarrow \mathbb{R}$ with $N>1$ are the gradient descent, the conjugate gradient method and the multidimensional Newton's method.
You calculate the search direction in all methods with different approaches and then calculate the stepsize. This can be done as a one-dimensional minimization problem.
Those steps are repeated until the algorithm reaches a desired accuracy.
