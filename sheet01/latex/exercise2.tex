\section{Linear regression}

\begin{table}[h]
    \centering
    \caption{data points $(x,y)$}
    \label{tab:data_points}
    \begin{tabular}{c c c c c c c c c c c}
        \toprule
        x & 0 & 2.5 & -6.3 & 4 & -3.2 & 5.3 & 10.1 & 9.5 & -5.4 & 12.7 \\
        y & 4 & 4.3 & -3.9 & 6.5 & 0.7 & 8.6 & 13 & 9.9 & -3.6 & 15.1 \\
        \bottomrule
    \end{tabular}
\end{table}

% a) and b)
\subsection{Problem formulation \& 2.b) Tranformation into a system with a square matrix}

We want to calculate a linear regression in the form 
\begin{equation}
    y(x) = m x + n \, .
\end{equation} 
For this, a naiv approach would be to solve a SLE
\begin{equation}
    \vec{y} = 
    \underbrace{
        \begin{pmatrix}
            | & 1 \\
            \vec{x} & \vdots \\
            | & 1
        \end{pmatrix}
    }_{\symbf{A} \coloneqq}
    \underbrace{
        \begin{pmatrix}
            m \\
            n
        \end{pmatrix}
    }_{\vec{m} \coloneqq} \, ,
\end{equation}
but this SLE probably has no solution. 
Instead we want to calculate the minimum of the quadratic error
\begin{equation}
    R \coloneqq \sum_i (\symbf{A} \vec{m} - \vec{y})_i^2 \, .
\end{equation}
As shown in the lecture, this "least-square minimum" is calculated by solving
\begin{equation}
    \label{eq:system}
    \underbrace{
        \symbf{A}^T \symbf{A}
    }_{\symbf{P} \coloneqq}
    \vec{m} = 
    \underbrace{
        \symbf{A}^T \vec{y}
    }_{\vec{b} \coloneqq}
\end{equation}
where $\symbf{P}$ is a square matrix.

\setcounter{subsection}{2}
% c) and d)
\subsection{Carrying out the linear regression \& 2.d) Plotting the linear regression}

The given data points in \autoref{tab:data_points} yield the following:

\begin{align*}
    \symbf{P} &=
    \begin{pmatrix}
        482.98 & 29.2 \\
        29.2 & 10
    \end{pmatrix}\\
    \vec{b} &=
    \begin{pmatrix}
        541.22 \\
        54.6
    \end{pmatrix}
\end{align*}

Now the LU decomposition of $\symbf{P}$ yields
\begin{align*}
    \symbf{P}_{LU} &=
    \begin{pmatrix}
        1 & 0 \\
        0 & 1
    \end{pmatrix} \\
    \symbf{L} &=
    \begin{pmatrix}
        1 & 0 \\
        0.060458 & 1
    \end{pmatrix} \\
    \symbf{U} &=
    \begin{pmatrix}
        482.98 & 29.2 \\
        0 & 8.23463
    \end{pmatrix} \, .
\end{align*}

And solving the SLE \autoref{eq:system} yields
\begin{equation}
    \vec{m} = 
    \begin{pmatrix}
        m \\ 
        n
    \end{pmatrix}
    =
    \begin{pmatrix}
        0.959951 \\ 
        2.65694
    \end{pmatrix} \, .
\end{equation}

The linear regression is plotted in \autoref{fig:plot_exercise2} using these parameters $m$ and $n$.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{../code/build/plot_exercise2.pdf}
    \caption{Plot of the linear regression}
    \label{fig:plot_exercise2}
\end{figure}